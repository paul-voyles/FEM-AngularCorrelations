{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluctuation Electron Microscopy Data Analysis Example\n",
    "\n",
    "\n",
    "## 0.0 Introduction\n",
    "This notebook shows you how to perform a fluctuation electron mircoscopy (FEM) analysis for a Zr-Cu-Al Sample.  Most of this analysis is done in [pyXEM](https://github.com/pyxem/pyxem) version 13.3. As a package in development there may be some breaking changes with each sucessive itereation.  pyxXEM is an extension of [hyperspy](https://hyperspy.org/hyperspy-doc/current/index.html), so reading some of their documentation is a good place to start in many cases. \n",
    "\n",
    "In this notebook we will focus on calculating $V_\\Omega (k)$ which is defined as,\n",
    "\n",
    "Equation 1:   $V_\\Omega (k) = \\frac{\\langle I{^2}(k) \\rangle{_r} - \\langle I(k)\\rangle_r^2}{\\langle I(k)\\rangle_r^2 } - \\frac{G}{\\langle I(k)\\rangle_r}$,\n",
    "\n",
    "where $I(k)$ is the diffracted electron intensity averaged over the polar angle at constant scattering vector magnitude $k$, $\\langle \\rangle_r$ indicates averaging over probe positions $r$, and $G$ is the gain of the electron camera in digital counts / beam electron. The first term is the definition of the variance, and the second term is a correction to the variance for Poisson noise in the data.\n",
    "\n",
    "(There are several different possible variance signals. Here, we use the notation from Daulton, <i>et al.</i> Ultramicroscopy <b>110</b>, 1279–1289 (2010), DOI: [10.1016/j.ultramic.2010.05.010](https://doi.org/10.1016/j.ultramic.2010.05.010).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hyperspy.api as hs\n",
    "import glob\n",
    "import pyxem\n",
    "from skimage.measure import EllipseModel\n",
    "from hyperspy.signal import BaseSignal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Loading and Visualizing the Data \n",
    "\n",
    "The example data is a 10x10 STEM dataset with large spacing betweent the probe positions. A HAADF image is also included to use as a reference for the thickness filtering.\n",
    "\n",
    "The data is currently in the `.hspy` format which is hyperspy's open hdf5 file format.  Hyperspy can load many different file types, a complete list of which is given [here](https://hyperspy.org/hyperspy-doc/current/user_guide/io.html#supported-formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = glob.glob('Data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = hs.load(signals[1], signal_type=\"ElectronDiffraction2D\")\n",
    "signal.set_signal_type(\"ElectronDiffraction2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# change the plotting backend to \"notebook\" to make it intereactive\n",
    "%matplotlib inline\n",
    "signal.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Computing the Annular Average of Every Diffraction Pattern\n",
    "\n",
    "The first step in computing $V_\\Omega(k)$ is computing the annular average of each diffraction pattern, converting in effect from a 2D pattern $I(k_x, k_y)$ to a 1D pattern $I(k)$. This step involves finding the center of the pattern, correcting for any ellipticity, then calculating the average.\n",
    "\n",
    "The following methods below are useful for determing the center of a diffraction pattern if there is a beam stop included in the data.  The intent is to include these methods into pyxem (see https://github.com/pyxem/pyxem/pull/769) but they currently are not included.  If the data don't have a beam stop the [`diffraction_signal.center_direct_beam()`](https://github.com/pyxem/pyxem/blob/de140bb5dc19c0e9d1a91a725c656c2bd2c056d0/pyxem/signals/diffraction2d.py#L698) methods are better.\n",
    "\n",
    "The method below also provides some ability to correct for ellipticty in the diffraction pattern. In general the best way to see if there is any ellipticity in a diffraction pattern is to visualize at the polar unwrapped diffraction pattern, which is done below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_positions(signal,\n",
    "                      mask=None,\n",
    "                      num_points=5000,\n",
    "                      ):\n",
    "    \"\"\" Gets the top num_points pixels in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    signal: BaseSignal\n",
    "        The signal which we want to find the max positions for.\n",
    "    mask: np.array\n",
    "        A mask to be applied to the data for values to ignore\n",
    "    num_points: int\n",
    "        The number of points to be\n",
    "    \"\"\"\n",
    "    if isinstance(signal, BaseSignal):\n",
    "        data = signal.data\n",
    "    else:\n",
    "        data = signal\n",
    "    i_shape = np.shape(data)\n",
    "    flattened_array = data.flatten()\n",
    "    if mask is not None:\n",
    "        flattened_mask = mask.flatten()\n",
    "        flattened_array[flattened_mask]=0\n",
    "    # take top 5000 points make sure exclude zero beam\n",
    "    indexes = np.argsort(flattened_array)\n",
    "    cords = np.array([np.floor_divide(indexes[-num_points:], i_shape[1]),\n",
    "             np.remainder(indexes[-num_points:], i_shape[1])]) # [x axis (row),y axis (col)]\n",
    "    return cords.T\n",
    "\n",
    "def determine_ellipse(signal,\n",
    "                      mask=None,\n",
    "                      num_points=1000,\n",
    "                      **kwargs,\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    This method starts by taking some number of points which are the most intense\n",
    "    in the signal.  It then takes those points and guesses some starting parameters\n",
    "    for the `get_ellipse_model_ransac_single_frame` function. From there it will try\n",
    "    to determine the ellipse parameters.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    signal: Signal2D\n",
    "        The signal of interest\n",
    "    mask: Array-like\n",
    "        The mask to be applied to the data.  All of the masked values are ignored\n",
    "    num_points: int\n",
    "        The number of points to consider\n",
    "    guess_starting_params: bool\n",
    "        If True then the starting parameters will be guessed based on the points determined.\n",
    "    **kwargs:\n",
    "        Any other keywords for ` get_ellipse_model_ransac_single_frame`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    center: (x,y)\n",
    "        The center of the diffraction pattern\n",
    "    affine:\n",
    "        The affine transformation to make the diffraction pattern circular.\n",
    "    \"\"\"\n",
    "    pos = get_max_positions(signal,\n",
    "                            mask=mask,\n",
    "                            num_points=num_points)\n",
    "    e = EllipseModel()\n",
    "    converge = e.estimate(data=pos)\n",
    "    el = e\n",
    "    if el is not None:\n",
    "        affine = ellipse_to_affine(el.params[3],el.params[2], el.params[4])\n",
    "        center = (el.params[0],el.params[1])\n",
    "        return center, affine\n",
    "    else:\n",
    "        print(\"Ransac Ellipse detection did not converge\")\n",
    "        return None\n",
    "\n",
    "def ellipse_to_affine(major, minor, rot):\n",
    "    if major < minor:\n",
    "        print(\"major<minor\")\n",
    "        temp = major\n",
    "        major = minor\n",
    "        minor = temp\n",
    "        rot=rot+(np.pi/2)\n",
    "\n",
    "    Q = [[np.cos(rot), -np.sin(rot), 0],\n",
    "         [np.sin(rot), np.cos(rot), 0],\n",
    "         [0, 0, 1]]\n",
    "    S = [[1, 0, 0],\n",
    "         [0, major / minor, 0],\n",
    "         [0, 0, 1]]\n",
    "    C = np.matmul(np.matmul(Q, S), np.transpose(Q))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center, affine =determine_ellipse(signal.sum())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.beam_energy=200\n",
    "signal.unit=\"k_nm^-1\"\n",
    "signal.set_ai(center= center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.get_azimuthal_integral2d(npt=100).sum().isig[:,2.:8.].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Calculating the Variance for Constant Thickness Samples\n",
    "\n",
    "If the TEM sample is known to have constant thickness, computing the variance from this point is quite easy.  The `get_variance` method can easily calculate the variance using the Azimuthal_Integrator set above with the `set_ai` method. \n",
    "\n",
    "Note that the DQE parameter isn't really the best description for that parameter.  A more accureate description is the gain of the detector.  It is used to count the number of electrons which hit the detector.  That is then used for the Possion noise correction. If the data are already calibrated in units of electron counts, use a gain of 1.\n",
    "\n",
    "<b>NOTE FOR CARTER: IF THE PARAMETER IS THE GAIN AND NOT THE DQE, WHY IN THE WORLD IS IT CALLED THE DQE? JUST CHANGE THE PARAMETER NAME!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(signal.get_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.get_variance(npt=50,dqe=4.2, radial_range=(3.0,5.75)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Calculating the Variance for Sample with Varying Thickness\n",
    "\n",
    "If the TEM sample varies in thickness by more than a few nm over the entire dataset, thickness-related differences in the diffracted intensity $I(k)$ will dominate structure-related differences and therefore dominate $V_\\Omega(k)$. This effect can be avoided by using the HAADF signal or the high-angle scattering within the diffraction pattern to determine the local sample thickness, grouping the diffraction patterns into bins of nearly constant thickness, and only computing $V_\\Omega(k)$ for data inside a single bin. The $V_\\Omega(k)$ from different thickness bins then can be averaged together. See Hwang and Voyles Microscopy and Microanalysis <b>17</b>, 67–74 (2011), DOI: [10.1017/S1431927610094109](https://doi.org/10.1017/S1431927610094109) and Li <i>et al.</i> Microscopy and Microanalysis <b>20</b>, 1605–1618 (2014). DOI: [10.1017/s1431927614012756](https://doi.org/10.1017/s1431927614012756) for more details.\n",
    "\n",
    "Thickness filtering is not currently built into `pyxem` but preforming some thickness filtering isn't terrribly difficult using the framework available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have already saved the simultaneously-acquired HAADF image along side the dataset. We can see this here...\n",
    "signal.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haadf = hs.signals.Signal2D(signal.metadata.HAADF.intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness = ((haadf -26265)/440.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thickness_filter(signal, thickness, bins):\n",
    "    masks = [np.logical_and(bins[i]<thickness, bins[i+1]>thickness) for i in range(len(bins)-1)]\n",
    "    filtered = [hs.signals.Signal2D(signal.data[m.data,:,:]) for m in masks]\n",
    "    for f in filtered:\n",
    "        f.set_signal_type(\"electron_diffraction\")\n",
    "        f.axes_manager.signal_axes= signal.axes_manager.signal_axes\n",
    "        f.metadata = signal.metadata\n",
    "    return filtered, thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(np.min(thickness, axis=(0,1)), np.max(thickness, axis=(0,1)), num=2+1)\n",
    "filtered, thickness = thickness_filter(signal, thickness, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = [f.get_variance(npt=50, dqe=4.2, radial_range=(3.0, 5.7)) for f in filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.plot.plot_spectra(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Example \n",
    "## (WARNING! This might not work in Binder because of Memory Constraints)\n",
    "\n",
    "The small example data set analyzed above doesn't contain enough data for a statistically rigorous estimate of the variance. 100 diffraction patterns split into different thicknesses just isn't enough data!\n",
    "\n",
    "Below we have loaded 10 seperate positions each with a 10x10 dataset collected.  This gets closer to being enough data for a good estimate of $V_\\Omega(k)$ with thickness filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = glob.glob('Data/*')\n",
    "signals = hs.load(signals, signal_type=\"electron_diffraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in signals:\n",
    "    center, affine =determine_ellipse(s.sum())\n",
    "    s.beam_energy=200\n",
    "    s.unit=\"k_nm^-1\"\n",
    "    s.set_ai(center= center)\n",
    "    haadf = hs.signals.Signal2D(s.metadata.HAADF.intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = [s.get_variance(npt=50, dqe=4.2, radial_range=(3.0, 5.7) )for s in signals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs =plt.subplots(1,1,figsize=(10,5))\n",
    "\n",
    "error = np.std([v.data for v in var])/np.sqrt(10)\n",
    "mean_val = np.mean(var).data\n",
    "axs.plot(np.linspace(3.0,5.7,50), mean_val)\n",
    "axs.fill_between(np.linspace(3.0,5.7,50),mean_val-error, mean_val+error, alpha=.2)\n",
    "axs.set(xlim=(3.0,5.7), ylabel=\"Variance\", xlabel=\"k, nm $^{-1}$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Methods for Calculating Variance\n",
    "\n",
    "\n",
    "Additional methods for calculating varaince are also available as described in Daulton <i>et al.</i> Ultramicroscopy <b>110</b>, 1279–1289, https://doi.org/10.1016/j.ultramic.2010.05.010 Nanobeam diffraction fluctuation electron microscopy technique for structural characterization of disordered materials-Application to Al<sub>88-x</sub>Y<sub>7</sub>Fe<sub>5</sub>Ti<sub>x</sub> metallic glasses. All of the different variance signals are straightforward to calculate using `hyperspy` and `pyxem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs =plt.subplots(1,3,figsize=(15,5))\n",
    "gain=4.2\n",
    "for method,ax in zip([\"Omega\", \"r\", \"re\"],axs):\n",
    "    var = [s.get_variance(npt=50, dqe=gain, radial_range=(3.0, 5.7), method=method)for s in signals]\n",
    "    error = np.std([v.data for v in var])/np.sqrt(10)\n",
    "    mean_val = np.mean(var).data\n",
    "    ax.plot(np.linspace(3.0,5.7,50), mean_val)\n",
    "    ax.fill_between(np.linspace(3.0,5.7,50),mean_val-error, mean_val+error, alpha=.2)\n",
    "    ax.set(xlim=(3.0,5.7), ylabel=\"Variance\", xlabel=\"k, nm $^{-1}$\")\n",
    "    ax.title.set_text(method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
